#!/bin/bash
set -e
export PATH="$PATH:/usr/local/go/bin"
# ----------------------------
# Environment & Validation Functions
# ----------------------------

#--- Github Settings to pull the code (can be overridden via env)
GITHUB_USER="${GITHUB_USER:-}"  # Set via env or leave empty
GITHUB_TOKEN="${GITHUB_TOKEN:-}"  # Set via env or leave empty

#--- harbor settings (can be overridden via env)
EXPOSED_HARBOR_IP="${EXPOSED_HARBOR_IP:-127.0.0.1}"
EXPOSED_HARBOR_PORT="${EXPOSED_HARBOR_PORT:-8081}"

#--- branch details (can be overridden via env)
DEV_REPO_BRANCH="${DEV_REPO_BRANCH:-dev-sprint-6}"
WFM_IP="${WFM_IP:-127.0.0.1}"
WFM_PORT="${WFM_PORT:-8082}"

# Device type configuration (can be overridden via env)
DEVICE_TYPE="${DEVICE_TYPE:-k3s}"  # Options: "k3s" or "docker"

#--- Registry settings (can be overridden via env)
REGISTRY_URL="${REGISTRY_URL:-http://${EXPOSED_HARBOR_IP}:${EXPOSED_HARBOR_PORT}}"
REGISTRY_USER="${REGISTRY_USER:-admin}"
REGISTRY_PASS="${REGISTRY_PASS:-Harbor12345}"

# variables for observability stack
NAMESPACE_OBSERVABILITY="observability"
PROMTAIL_RELEASE="promtail"
OTEL_RELEASE="otel-collector"


# Pinned software versions (can be overridden via env)
DOCKER_VERSION="${DOCKER_VERSION:-29.1.2}"
DOCKER_COMPOSE_VERSION="${DOCKER_COMPOSE_VERSION:-5.0.0}"

# Stable version as of December 2024
K3S_VERSION="${K3S_VERSION:-v1.31.4+k3s1}"

export GOINSECURE='github.com/margo/*'
export GONOPROXY='github.com/margo/*'
export GONOSUMDB='github.com/margo/*'
export GOPRIVATE='github.com/margo/*'

validate_pre_required_vars() {
  local required_vars=("GITHUB_USER" "GITHUB_TOKEN" "DEV_REPO_BRANCH" "WFM_IP" "WFM_PORT")
  for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
      echo "Error: Required environment variable $var is not set"
      exit 1
    fi
  done
}

validate_start_required_vars() {
  local required_vars=("WFM_IP" "WFM_PORT")
  for var in "${required_vars[@]}"; do
    if [ -z "${!var}" ]; then
      echo "Error: Required environment variable $var is not set"
      exit 1
    fi
  done
}

# ----------------------------
# Go Installation Functions
# ----------------------------
install_basic_utilities() {
  sudo apt update -y
  sudo apt install -y curl git dos2unix build-essential gcc libc6-dev
  echo "Installation complete: curl, git, and build tools installed."

  # Only install Helm for k3s device type
  if [ "$DEVICE_TYPE" = "k3s" ]; then
    INSTALL_HELM_V3_15_1=true
    HELM_VERSION="3.15.1"
    HELM_TAR="helm-v${HELM_VERSION}-linux-amd64.tar.gz"
    HELM_BIN_DIR="/usr/local/bin"
    install_helm
    echo "‚úÖ Helm installed for k3s device"
  else
    echo "‚ÑπÔ∏è Skipping Helm installation for docker device type"
  fi
}




install_docker_and_compose() {
  cd $HOME
  
  # Define pinned versions
  local DOCKER_VERSION="${DOCKER_VERSION:-29.1.2}"
  local DOCKER_COMPOSE_VERSION="${DOCKER_COMPOSE_VERSION:-5.0.0}"
  local UBUNTU_CODENAME=$(lsb_release -cs 2>/dev/null || echo "noble")
  
  # Install Docker if not present
  if ! command -v docker >/dev/null 2>&1; then
    echo "Docker not found. Installing Docker ${DOCKER_VERSION}..."
    
    # Remove old Docker packages
    apt-get remove -y docker docker-engine docker.io containerd runc 2>/dev/null || true
    
    # Add Docker's official GPG key and repository
    apt-get update
    apt-get install -y ca-certificates curl
    install -m 0755 -d /etc/apt/keyrings
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
    chmod a+r /etc/apt/keyrings/docker.asc
    
    echo \
      "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
      ${UBUNTU_CODENAME} stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null
    
    apt-get update
    
    # Install specific Docker version
    apt-get install -y \
      docker-ce=5:${DOCKER_VERSION}-1~ubuntu.24.04~${UBUNTU_CODENAME} \
      docker-ce-cli=5:${DOCKER_VERSION}-1~ubuntu.24.04~${UBUNTU_CODENAME} \
      containerd.io=1.7.27-1 \
      docker-buildx-plugin=0.23.0-1~ubuntu.24.04~${UBUNTU_CODENAME}
    
    usermod -aG docker $USER
    echo "‚úÖ Docker ${DOCKER_VERSION} installed successfully"
  else
    echo 'Docker already installed.'
    CURRENT_DOCKER_VERSION=$(docker version --format '{{.Server.Version}}' 2>/dev/null)
    echo "Current Docker version: $CURRENT_DOCKER_VERSION"
  fi

  # Install specific Docker Compose plugin version
  if ! apt list --installed 2>/dev/null | grep -q docker-compose-plugin; then
    echo "Installing Docker Compose plugin ${DOCKER_COMPOSE_VERSION}..."
    apt-get update
    apt-get install -y docker-compose-plugin=${DOCKER_COMPOSE_VERSION}-1~ubuntu.24.04~${UBUNTU_CODENAME}
  else
    echo 'Docker Compose plugin already installed.'
    CURRENT_COMPOSE_VERSION=$(docker compose version --short 2>/dev/null | sed 's/v//')
    echo "Current Docker Compose version: v$CURRENT_COMPOSE_VERSION"
  fi
  
  # Remove old standalone binaries
  echo 'Cleaning up old docker-compose binaries...'
  rm -f /usr/local/bin/docker-compose /usr/bin/docker-compose 2>/dev/null || true
  
  # Verify versions
  echo ""
  echo "Docker version:"
  docker version | grep -E "Version|API version" | head -4
  echo ""
  echo "Docker Compose version:"
  docker compose version
  
  # Hold packages at current versions
  echo "üîí Holding Docker packages at current versions..."
  apt-mark hold docker-ce docker-ce-cli docker-compose-plugin containerd.io docker-buildx-plugin
  
  echo "‚úÖ Docker ${DOCKER_VERSION} and Docker Compose v${DOCKER_COMPOSE_VERSION} ready"
}




# Helm install/uninstall
install_helm() {
  cd $HOME
  if [ "${INSTALL_HELM_V3_15_1}" == "true" ]; then
    echo "Helm Setup"
    if command -v helm >/dev/null 2>&1 && [[ "$(helm version --short | grep -oE '[0-9]+\.[0-9]+\.[0-9]+')" == "${HELM_VERSION}" ]]; then
        echo "Helm version ${HELM_VERSION} is already installed. Skipping."
    else
        echo "Downloading Helm version ${HELM_VERSION}..."
        if ! wget -q "https://get.helm.sh/${HELM_TAR}" ; then
            echo "Failed to download Helm."
            exit 1
        fi
        echo "Extracting Helm..."
        if ! tar -xzf "${HELM_TAR}" ; then
            echo "Failed to extract Helm tarball."
            exit 1
        fi
        echo "Moving Helm to ${HELM_BIN_DIR}..."
        if ! sudo mv "linux-amd64/helm" "${HELM_BIN_DIR}/" ; then
            echo "Failed to move Helm."
            exit 1
        fi
        echo "Helm binary moved successfully."
        echo "Cleaning up..."
        rm "${HELM_TAR}"
        rm -rf linux-amd64/
    fi
  fi
}


install_go() {
  if which go >/dev/null 2>&1; then
    echo 'Go already installed, skipping installation';
    go version;
  else
    echo 'Go not found, installing...';
    # rm -rf /usr/local/go /usr/bin/go
    wget "https://go.dev/dl/go1.23.2.linux-amd64.tar.gz" -O go.tar.gz;
    tar -C /usr/local -xzf go.tar.gz;
    rm go.tar.gz
    export PATH="$PATH:/usr/local/go/bin";
    which go;
    go version;
  fi
}

# ----------------------------
# Repository Functions
# ----------------------------
clone_dev_repo() {
  echo "Cloning sandbox on ($VM2_HOST)..."
  cd $HOME
  sudo rm -rf sandbox
  git clone "https://${GITHUB_USER}:${GITHUB_TOKEN}@github.com/margo/sandbox.git"
  cd sandbox
  git checkout ${DEV_REPO_BRANCH}
  cd ..
}

# ----------------------------
# Configuration Functions
# ----------------------------
update_agent_sbi_url() {
  echo 'Updating wfm.sbiUrl in workload-fleet-management-client config ...'
  sed -i "s|sbiUrl:.*|sbiUrl: https://$WFM_IP:$WFM_PORT/v1alpha2/margo|" "$HOME/sandbox/poc/device/agent/config/config.yaml"
}

# ----------------------------
# K3s Installation Functions
# ----------------------------
check_k3s_installed() {
  if command -v k3s >/dev/null 2>&1; then
    echo 'k3s already installed.'
    
    # Check current version
    CURRENT_K3S_VERSION=$(k3s --version | grep -oE 'v[0-9]+\.[0-9]+\.[0-9]+\+k3s[0-9]+' | head -1)
    echo "Current k3s version: $CURRENT_K3S_VERSION"
    
    if [ "$CURRENT_K3S_VERSION" != "$K3S_VERSION" ]; then
      echo "‚ö†Ô∏è  Expected k3s version: $K3S_VERSION"
      echo "‚ÑπÔ∏è  To upgrade/downgrade, uninstall current k3s and run installation again"
    fi
    
    return 0
  else
    return 1
  fi
}

install_k3s_dependencies() {
  echo 'Installing k3s dependencies...'
  sudo apt update
  sudo apt install -y curl
}

install_k3s() {
  if ! check_k3s_installed; then
    echo "Installing k3s ${K3S_VERSION}..."
    install_k3s_dependencies
    
    # Install specific k3s version
    curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION="${K3S_VERSION}" sh -
    
    echo "‚úÖ k3s ${K3S_VERSION} installed successfully"
  fi
}

verify_k3s_status() {
  echo 'Verifying k3s status...'
  sudo systemctl status k3s --no-pager || true
  sudo k3s kubectl get nodes || true
  
  # Show installed version
  echo ""
  echo "Installed k3s version:"
  k3s --version | head -1
}

setup_kubeconfig() {
  echo 'Setting up kubeconfig...'
  mkdir -p "$HOME/.kube"
  sudo cp /etc/rancher/k3s/k3s.yaml "$HOME/.kube/config"
  sudo chown $(id -u):$(id -g) "$HOME/.kube/config"
  export KUBECONFIG="$HOME/.kube/config"
  echo 'Kubeconfig setup complete.'
  kubectl get nodes || true
}

setup_k3s() {
  install_k3s
  verify_k3s_status
  setup_kubeconfig
  echo "‚úÖ k3s ${K3S_VERSION} setup complete"
}

install_vim() {
  echo "[INFO] Checking if Vim editor is installed..."
  if command -v vim >/dev/null 2>&1; then
    echo "[INFO] Vim is already installed."
    return
  fi

  echo "[INFO] Installing Vim..."
  if command -v apt >/dev/null 2>&1; then
    sudo apt update -y
    sudo apt install -y vim
  else
    sudo yum install -y vim || sudo dnf install -y vim
  fi

  echo "[SUCCESS] Vim installed and ready to use."
}


install_and_enable_ssh() {
  echo "[INFO] Checking OS type..."
  
  # Detect package manager
  if command -v apt >/dev/null 2>&1; then
    OS="debian"
    echo $OS
  elif command -v yum >/dev/null 2>&1 || command -v dnf >/dev/null 2>&1; then
    OS="rhel"
    echo $OS
  else
    echo "[ERROR] Unsupported OS. Only Debian/Ubuntu & RHEL/CentOS supported."
    return 1
  fi

  echo "[INFO] Installing OpenSSH Server..."
  if [ "$OS" = "debian" ]; then
    sudo apt update -y
    sudo apt install -y openssh-server
  else
    sudo yum install -y openssh-server || sudo dnf install -y openssh-server
  fi

  echo "[INFO] Enabling and starting SSH service..."
  UNIT=$(systemctl list-unit-files | awk '/^ssh\.service/ {print "ssh"} /^sshd\.service/ {print "sshd"}' | head -n1)

  if [ -z "$UNIT" ]; then
    echo "[ERROR] SSH service unit not found."
    return 1
  fi

  sudo systemctl enable "$UNIT"
  sudo systemctl restart "$UNIT"

  echo "[INFO] Verifying SSH status:"
  sudo sudo systemctl status ssh --no-pager || sudo systemctl status sshd
  echo "[SUCCESS] SSH service installed and running."
}

#-----------------------------------------------------------------
# Device Workload Fleet management Client Runtime Configuration update based on Docker or K8s
#-----------------------------------------------------------------

enable_kubernetes_runtime() {
  CONFIG_FILE="$HOME/sandbox/helmchart/config/config.yaml"
  echo "Enabling Kubernetes section in config.yaml for ServiceAccount authentication..."
  sed -i \
  -e 's/^[[:space:]]*#\s*-\s*type:\s*KUBERNETES/- type: KUBERNETES/' \
  -e 's/^[[:space:]]*#\s*kubernetes:/  kubernetes:/' \
  -e 's/^[[:space:]]*-\s*type:\s*DOCKER/  # - type: DOCKER/' \
  -e 's/^[[:space:]]*docker:/  # docker:/' \
  -e 's/^[[:space:]]*url:/  # url:/' \
  "$CONFIG_FILE"
  
 # Fix certificate paths
  sed -i \
    -e 's|pubCertPath:.*|pubCertPath: /certs/device-public.crt|' \
    -e 's|path: "./config/device-private.key"|path: "/certs/device-private.key"|' \
    -e 's|path: "./config/ca-cert.pem"|path: "/certs/ca-cert.pem"|' \
    "$CONFIG_FILE"				 
  
  # Set kubeconfigPath to empty string for ServiceAccount authentication
  sed -i 's|kubeconfigPath:.*|kubeconfigPath: ""|' "$CONFIG_FILE"
  
  echo "‚úÖ Kubernetes runtime enabled with ServiceAccount authentication"
}

enable_docker_runtime() {
  CONFIG_FILE="$HOME/sandbox/docker-compose/config/config.yaml"
 echo "Enabling docker section in config.yaml..."
 sed -i \
  -e 's/^[[:space:]]*#\s*- type: DOCKER/  - type: DOCKER/' \
  -e 's/^[[:space:]]*#\s*docker:/    docker:/' \
  -e 's/^[[:space:]]*#\s*url:/      url:/' \
  -e 's/^[[:space:]]*- type: KUBERNETES/  # - type: KUBERNETES/' \
  -e 's/^[[:space:]]*kubernetes:/    # kubernetes:/' \
  -e 's/^[[:space:]]*kubeconfigPath:/      # kubeconfigPath:/' \
  "$CONFIG_FILE"
}

# ----------------------------
# Device Workload Fleet management Client Build Functions
# ----------------------------
build_device_agent_docker() {
  cd "$HOME/sandbox"
  echo 'Checking if workload-fleet-management-client image already exists...'

# Check if the image exists
  if docker images -q margo.org/workload-fleet-management-client:latest | grep -q .; then
    echo "workload-fleet-management-client image already exists. Skipping build."
  else
    echo 'Building workload-fleet-management-client...'
    docker build -f poc/device/agent/Dockerfile . -t margo.org/workload-fleet-management-client:latest
  fi
  echo 'workload-fleet-management-client image build complete.'     
}


# ----------------------------
# Device Workload Fleet management Client Service Functions
# ----------------------------

start_device_agent_docker_service() {
  echo 'Starting workload-fleet-management-client...'
  cd "$HOME/sandbox/docker-compose"
  mkdir -p config
  
 if [ -f "$HOME/certs/device-private.key" ] && [ -f "$HOME/certs/device-public.crt" ] && [ -f "$HOME/certs/device-ecdsa.crt" ] && [ -f "$HOME/certs/device-ecdsa.key" ] && [ -f "$HOME/certs/ca-cert.pem" ]; then
    echo "Creating TLS secrets..."
    cp "$HOME/certs/device-private.key"  ./config
    cp "$HOME/certs/device-public.crt"   ./config
    cp "$HOME/certs/device-ecdsa.key"    ./config
    cp "$HOME/certs/device-ecdsa.crt"    ./config
    cp "$HOME/certs/ca-cert.pem"         ./config
    echo "Copied certs from \$HOME/certs to ./config"
      else
    echo "‚ùå device-start-failed: Required certificates missing in $HOME/certs (ca-cert.pem)"
        return 1 
  fi
  
  cp ../poc/device/agent/config/capabilities.json ./config/
  cp ../poc/device/agent/config/config.yaml ./config/

  mkdir -p data
  enable_docker_runtime
  docker compose up -d
}


stop_device_agent_service_docker() {
  echo "Stopping workload-fleet-management-client..."
  cd "$HOME/sandbox/docker-compose"
  docker compose down
  
  # Prompt user to delete /data folder
  echo ""
  echo "‚ö†Ô∏è  Warning: Deleting /data folder will require device re-onboarding"
  read -p "Do you want to delete data folder at $HOME/sandbox/docker-compose/data? (y/n): " delete_data
  
  if [[ "$delete_data" =~ ^[Yy]$ ]]; then
    echo "Deleting data folder..."
    if rm -rf "$HOME/sandbox/docker-compose/data"; then
      echo '‚úÖ Data folder deleted successfully'
      echo '‚ÑπÔ∏è Device re-onboarding will be required'
    else
      echo '‚ùå Failed to delete data folder'
    fi
  else
    echo '‚ÑπÔ∏è Data folder preserved'
  fi
}


build_start_device_agent_k3s_service() {
    cd "$HOME/sandbox"
    echo "Building and deploying workload-fleet-management-client on Kubernetes..."
    
    # Step 1: Build the Docker image if it doesn't exist
    echo "Checking if workload-fleet-management-client image exists..."
    if ! docker images | grep -q "margo.org/workload-fleet-management-client"; then
      echo "Building workload-fleet-management-client Docker image..."
      docker build -f poc/device/agent/Dockerfile . -t margo.org/workload-fleet-management-client:latest
      if [ $? -ne 0 ]; then
        echo "‚ùå Failed to build workload-fleet-management-client image"
        return 1
      fi
      echo "‚úÖ workload-fleet-management-client image built successfully"
    else
      echo "‚úÖ workload-fleet-management-client image already exists"
    fi
    
    # Step 2: Save and import image to k3s
    echo "Importing image to k3s cluster..."
    docker save -o workload-fleet-management-client.tar margo.org/workload-fleet-management-client:latest
    
						   
    if command -v k3s >/dev/null 2>&1; then
      k3s ctr -n k8s.io image import workload-fleet-management-client.tar
      echo "‚úÖ Image imported to k3s cluster"
    elif command -v ctr >/dev/null 2>&1; then
      ctr -n k8s.io image import workload-fleet-management-client.tar
      echo "‚úÖ Image imported to k3s cluster"
    else
      echo "‚ùå Neither k3s nor ctr command found"
      return 1
    fi
    
					   
    rm -f workload-fleet-management-client.tar
    
    # Step 3: Navigate to helmchart directory
    cd helmchart
    if [ $? -ne 0 ]; then
      echo "‚ùå Failed to navigate to helmchart directory"
      return 1
    fi
    
    # Step 4: Copy config files
    update_agent_sbi_url
    
    echo "Copying configuration files..."
    mkdir -p config
    cp -r ../poc/device/agent/config/* ./config
    
    if [ $? -eq 0 ]; then
      echo "‚úÖ Configuration files copied successfully"
    else
      echo "‚ùå Failed to copy configuration files"
      return 1
    fi
    
    enable_kubernetes_runtime
    
    # Step 5: Create secrets 
    if [ -d "$HOME/certs" ] && [ -f "$HOME/certs/device-private.key" ] && [ -f "$HOME/certs/device-public.crt" ] && [ -f "$HOME/certs/device-ecdsa.crt" ] && [ -f "$HOME/certs/device-ecdsa.key" ] && [ -f "$HOME/certs/ca-cert.pem" ]; then
        echo "Creating TLS secrets..."
   							 
        kubectl delete secret workload-fleet-management-client-certs --namespace=default 2>/dev/null || true
        
        kubectl create secret generic workload-fleet-management-client-certs \
            --from-file=device-private.key="$HOME/certs/device-private.key" \
            --from-file=device-public.crt="$HOME/certs/device-public.crt" \
            --from-file=device-ecdsa.key="$HOME/certs/device-ecdsa.key" \
            --from-file=device-ecdsa.crt="$HOME/certs/device-ecdsa.crt" \
            --from-file=ca-cert.pem="$HOME/certs/ca-cert.pem" \
            --namespace=default
        
        if [ $? -eq 0 ]; then
            echo "‚úÖ TLS secrets created successfully"
        else
            echo "‚ùå Failed to create TLS secrets"
            return 1
        fi
    else
        echo "‚ùå device-start-failed: Required certificates missing in $HOME/certs (ca-cert.pem)"
        return 1 
    fi

     
														
    # Step 6: Clean up old resources 
    echo "Cleaning up any existing resources..."
    kubectl delete clusterrole workload-fleet-management-client-role 2>/dev/null || true
    kubectl delete clusterrolebinding workload-fleet-management-client-binding 2>/dev/null || true
    
    helm uninstall workload-fleet-management-client -n default 2>/dev/null || true
    
											
    sleep 5

    # Step 7: Install with Helm 
    echo "Installing workload-fleet-management-client with persistent storage..."
    helm install workload-fleet-management-client . \
        --set serviceAccount.create=true \
        --set secrets.create=false \
        --set secrets.existingSecret=workload-fleet-management-client-certs \
        --set persistence.enabled=true \
        --set persistence.size=1Gi \
        --debug \
        --wait

    if [ $? -ne 0 ]; then
																	   
		
        echo "‚ùå Helm installation failed"
        return 1
    fi
    
    echo "‚úÖ Helm installation successful with persistent storage"
    
    # Step 8: Verify deployment (NO PATCHING NEEDED!)
    echo "üîç Verifying deployment..."
    
    # Verify RBAC permissions 
    if kubectl auth can-i create secrets --as=system:serviceaccount:default:workload-fleet-management-client-sa -n default | grep -q "yes"; then
      echo "‚úÖ RBAC permissions verified"
    else
      echo "‚ö†Ô∏è RBAC permissions may need manual verification"
    fi
    
    # Verify PVC (FIXED NAME)
												 
    if kubectl get pvc -n default | grep -q "workload-fleet-management-client-data"; then
      echo "‚úÖ Persistent volume claim created successfully"
      kubectl get pvc -n default | grep workload-fleet-management-client
    else
      echo "‚ö†Ô∏è PVC not found, checking for errors..."
      kubectl get events -n default --sort-by='.lastTimestamp' | tail -10
    fi
    
    echo "‚úÖ Device-workload-fleet-management-client deployed successfully"
    
    # Show deployment status
    echo ""
    echo "Deployment Summary:"
    kubectl get pods -n default | grep workload-fleet-management-client
    kubectl get serviceaccount -n default | grep workload-fleet-management-client
    kubectl get pvc -n default | grep workload-fleet-management-client
	
}

stop_device_agent_kubernetes() {
  echo "Stopping workload-fleet-management-client..."
  cd "$HOME/sandbox"

  # Ask user about PVC deletion FIRST (before Helm uninstall)
  DELETE_PVC=false
  read -p "Delete persistent data (PVC)? This will require re-onboarding. [y/N]: " -n 1 -r
  echo
  if [[ $REPLY =~ ^[Yy]$ ]]; then
    DELETE_PVC=true
    echo "‚ö†Ô∏è PVC will be deleted after uninstalling Helm release"
  else
    echo "‚ÑπÔ∏è Attempting to preserve PVC..."
    
    # Add Helm annotation to prevent PVC deletion during uninstall
    if kubectl get pvc workload-fleet-management-client-data -n default >/dev/null 2>&1; then
      kubectl annotate pvc workload-fleet-management-client-data -n default \
        "helm.sh/resource-policy=keep" \
        --overwrite 2>/dev/null && echo "‚úÖ PVC annotated for preservation" || echo "‚ö†Ô∏è Could not annotate PVC"
    else
      echo "‚ö†Ô∏è PVC not found, nothing to preserve"
    fi
  fi
  
  # Check if Helm release exists and uninstall
  if helm list -A | grep -q "workload-fleet-management-client"; then
    echo "Uninstalling workload-fleet-management-client Helm release..."
    helm uninstall workload-fleet-management-client --namespace default
    
    if [ $? -eq 0 ]; then
      echo "‚úÖ Device-workload-fleet-management-client Helm release uninstalled successfully"
    else
      echo "‚ùå Failed to uninstall Helm release"
      return 1
    fi
  else
    echo "No workload-fleet-management-client Helm release found, trying direct kubectl deletion..."
    kubectl delete deployment workload-fleet-management-client-deploy -n default 2>/dev/null || echo "No deployment found"
  fi
  
  # Clean up ServiceAccount and RBAC resources
  echo "Cleaning up ServiceAccount and RBAC resources..."
  kubectl delete serviceaccount workload-fleet-management-client-sa -n default 2>/dev/null || echo "No serviceaccount found"
  kubectl delete clusterrole workload-fleet-management-client-role 2>/dev/null || echo "No clusterrole found"
  kubectl delete clusterrolebinding workload-fleet-management-client-binding 2>/dev/null || echo "No clusterrolebinding found"
  
  # Clean up ConfigMaps and Secrets
  echo "Cleaning up configmaps and secrets..."
  kubectl delete configmap workload-fleet-management-client-cm -n default 2>/dev/null || echo "No configmap found"
  kubectl delete secret workload-fleet-management-client-certs -n default 2>/dev/null || echo "No secret found"
  
  # NOW handle PVC based on user choice
  if [ "$DELETE_PVC" = true ]; then
    echo "Deleting PVC as requested..."
    kubectl delete pvc workload-fleet-management-client-data -n default 2>/dev/null || echo "No PVC found"
    echo "‚úÖ PVC deleted - device will re-onboard on next start"
  else
    # Verify PVC was preserved
    if kubectl get pvc workload-fleet-management-client-data -n default >/dev/null 2>&1; then
      echo "‚úÖ PVC preserved successfully - device will resume with existing ID on next start"
      kubectl get pvc workload-fleet-management-client-data -n default
      
      # Remove the keep annotation for next deployment
      kubectl annotate pvc workload-fleet-management-client-data -n default \
        "helm.sh/resource-policy-" \
        2>/dev/null || true
    else
      echo "‚ö†Ô∏è PVC was not preserved (may have been deleted by Helm)"
      echo "   Ensure PVC template has 'helm.sh/resource-policy: keep' annotation"
    fi
  fi
  
  # Verify cleanup
  echo ""
  echo "Verifying cleanup..."
  if kubectl get pods -n default 2>/dev/null | grep -q "workload-fleet-management-client"; then
    echo "‚ö†Ô∏è Some workload-fleet-management-client pods may still be terminating"
    kubectl get pods -n default | grep workload-fleet-management-client
  else
    echo "‚úÖ All workload-fleet-management-client pods stopped"
  fi
  
  # Show remaining resources
  echo ""
  echo "Remaining workload-fleet-management-client resources:"
  kubectl get all,pvc,sa,cm,secrets -n default 2>/dev/null | grep workload-fleet-management-client || echo "‚úÖ No workload-fleet-management-client resources found (except possibly PVC if preserved)"
  
  # Check for remaining RBAC resources
  echo ""
  echo "Remaining RBAC resources:"
  kubectl get clusterroles,clusterrolebindings 2>/dev/null | grep workload-fleet-management-client || echo "‚úÖ No workload-fleet-management-client RBAC resources found"
  
  echo ""
  echo "‚úÖ Device-workload-fleet-management-client cleanup complete"
}



cleanup_device_agent() {
  echo "Cleaning up workload-fleet-management-client files..."
  
  # Check if workload-fleet-management-client container exists and remove it
  if docker ps -a --format "{{.Names}}" | grep -q "^workload-fleet-management-client$"; then
    echo "Stopping and removing workload-fleet-management-client container..."
    docker stop workload-fleet-management-client 2>/dev/null || true
    docker rm workload-fleet-management-client 2>/dev/null || true
    echo "Removed workload-fleet-management-client container"
  else
    echo "No workload-fleet-management-client container found"
  fi
 
  #If using Helm deployment, uninstall the release
  if helm list --short 2>/dev/null | grep -q "^workload-fleet-management-client$"; then
    echo "Uninstalling workload-fleet-management-client Helm release..."
    helm uninstall workload-fleet-management-client 2>/dev/null || true
    echo "Removed workload-fleet-management-client Helm release"
  else
    echo "No workload-fleet-management-client Helm release found"
  fi


}

add_container_registry_mirror_to_k3s() {
  echo "Configuring container registry mirror for k3s..."

  # ---------------------------------------------------
  # Load registry settings from environment variables
  # ---------------------------------------------------
  registry_url="${REGISTRY_URL:-http://${EXPOSED_HARBOR_IP}:${EXPOSED_HARBOR_PORT}}"
  registry_user="${REGISTRY_USER:-admin}"
  registry_password="${REGISTRY_PASS:-Harbor12345}"

  echo "Using registry mirror: $registry_url"
  echo "Using registry credentials: $registry_user / ******"
  # ---------------------------------------------------
  # Create k3s directory if needed
  # ---------------------------------------------------
  sudo mkdir -p /var/lib/rancher/k3s
  sudo mkdir -p /etc/rancher/k3s

  # Backup existing registries if present
  if [ -f /var/lib/rancher/k3s/registries.yml ]; then
    sudo cp /var/lib/rancher/k3s/registries.yml /var/lib/rancher/k3s/registries.yml.backup.$(date +%s)
    echo "‚úÖ Backed up /var/lib/rancher/k3s/registries.yml"
  fi

  # ---------------------------------------------------
  # Write the registry config
  # ---------------------------------------------------
  cat <<EOF | sudo tee /var/lib/rancher/k3s/registries.yml >/dev/null
mirrors:
  "${EXPOSED_HARBOR_IP}:${EXPOSED_HARBOR_PORT}":
    endpoint:
      - "${registry_url}"

configs:
  "${EXPOSED_HARBOR_IP}:${EXPOSED_HARBOR_PORT}":
    auth:
      username: "${registry_user}"
      password: "${registry_password}"
    tls:
      insecure_skip_verify: true
EOF

  sudo cp /var/lib/rancher/k3s/registries.yml /var/lib/rancher/k3s/registries.yaml
  sudo cp /var/lib/rancher/k3s/registries.yml /etc/rancher/k3s/registries.yml
  sudo cp /var/lib/rancher/k3s/registries.yml /etc/rancher/k3s/registries.yaml

  echo "‚úÖ Created k3s registry mirror configuration"
  # ---------------------------------------------------
	
  # Restart k3s
  # ---------------------------------------------------
  echo "Restarting k3s..."
  if sudo systemctl restart k3s; then
    echo "‚úÖ k3s restarted successfully"
  else
    echo "‚ùå Failed to restart k3s"
    return 1
  fi

  # Wait for k3s active
  echo "Waiting for k3s to come up..."
  for i in {1..30}; do
    if sudo systemctl is-active --quiet k3s; then
      echo "‚úÖ k3s is running"
      break
    fi
										 
    sleep 2
		
  done

  echo "Checking cluster..."
  if sudo k3s kubectl get nodes >/dev/null 2>&1; then
    echo "‚úÖ k3s cluster is responding"
  else
    echo "‚ö†Ô∏è k3s cluster not ready yet"
								
			
  fi

  echo "‚úÖ Registry mirror configuration completed."
}

# ----------------------------
# Main Orchestration Functions
# ----------------------------
install_prerequisites() {
  echo "Installing prerequisites: k3s and others ..."
  validate_pre_required_vars
  install_go
  install_vim
  install_and_enable_ssh
  install_basic_utilities
  install_docker_and_compose
  clone_dev_repo
  # Only install k3s for k3s device type
  if [ "$DEVICE_TYPE" = "k3s" ]; then
    setup_k3s
    add_container_registry_mirror_to_k3s
  fi
  
  echo 'prerequisites installation completed.'
}


start_device_agent_docker() {
  echo "Building and starting workload-fleet-management-client ..."
  validate_start_required_vars
  update_agent_sbi_url
  build_device_agent_docker
  start_device_agent_docker_service
   echo 'workload-fleet-management-client docker-container started'
}

start_device_agent_kubernetes() {
  echo "Building and starting workload-fleet-management-client with ServiceAccount authentication..."
  validate_start_required_vars
  build_start_device_agent_k3s_service
  echo '‚úÖ workload-fleet-management-client-pod started with ServiceAccount authentication'
}

stop_device_agent_docker() {
  echo "Stopping workload-fleet-management-client on VM2 ($VM2_HOST)..."
  stop_device_agent_service_docker
  echo "Device Workload Fleet management Client stopped"
}


uninstall_prerequisites() {
  cleanup_device_agent
}

show_status() {
  echo "Device Workload Fleet management Client Status:"
  echo "==================="
  
  # Check Docker first
  if docker ps --format "{{.Names}}" | grep -q "^workload-fleet-management-client$"; then
    echo "‚úÖ Device Workload Fleet management Client Docker Container is running."
    
    # Show container details
    echo "Container Details:"
    docker ps --filter "name=workload-fleet-management-client" --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}\t{{.Image}}"
    
    return 0
  fi
  
  # Check Kubernetes if Docker is not running (check workload-fleet-management-client namespace)
  if kubectl get pods -n default --no-headers 2>/dev/null | grep -q "workload-fleet-management-client"; then
  echo "‚úÖ Device Workload Fleet management Client Kubernetes Pod is running."
  
  # Show pod details
  echo "Pod Details:"
  kubectl get pods -n default -o wide | grep -E "(NAME|workload-fleet-management-client)"
  
  # Add ServiceAccount verification
  echo "ServiceAccount Details:"
  kubectl get serviceaccount -n default | grep workload-fleet-management-client || echo "No ServiceAccount found"
  
  return 0
  fi
  
  # If neither is running
  echo "‚ùå Device Workload Fleet management Client is not running on Docker or Kubernetes."
  echo "Available workload-fleet-management-client containers:"
  docker ps --format "table {{.Names}}\t{{.Status}}" | grep -E "(NAMES|workload-fleet-management-client)" || echo "No workload-fleet-management-client containers found"

  
  if command -v kubectl >/dev/null 2>&1; then
    echo ""
    echo "Available pods in workload-fleet-management-client namespace:"
    kubectl get pods -n default --no-headers 2>/dev/null | head -5 || echo "No workload-fleet-management-client namespace or pods found"
    
    
  fi
}




function install_promtail() {
  echo "üì¶ Installing Promtail to push logs to Loki at $WFM_IP..."

  cat <<EOF > promtail-values.yaml
config:
  server:
    http_listen_port: 9080
    grpc_listen_port: 0

  positions:
    filename: /tmp/positions.yaml

  clients:
    - url: http://${WFM_IP}:32100/loki/api/v1/push

  scrape_configs:
    - job_name: pod-logs
      static_configs:
        - targets:
            - localhost
          labels:
            job: podlogs
            __path__: /var/log/pods/*/*/*.log
EOF

  helm repo add grafana https://grafana.github.io/helm-charts
  helm repo update

  helm install $PROMTAIL_RELEASE grafana/promtail --version 6.17.1 -f promtail-values.yaml --namespace $NAMESPACE_OBSERVABILITY

  echo "‚úÖ Promtail installed and configured to push logs to Loki"
}

function install_otel_collector() {
  echo "üì° Installing OTEL Collector to send metrics and traces to WFM node..."

  cat <<EOF > otel-values.yaml
mode: deployment
image:
  repository: otel/opentelemetry-collector-contrib

extraEnvs:
  - name: KUBE_NODE_NAME
    valueFrom:
      fieldRef:
        fieldPath: spec.nodeName

config:
  receivers:
    otlp:
      protocols:
        http:
          endpoint: 0.0.0.0:4318
        grpc:
          endpoint: 0.0.0.0:4317

    hostmetrics:
      collection_interval: 30s
      scrapers:
        cpu:
        memory:
        disk:
        filesystem:
        load:
        network:
        processes:
        paging:

    kubeletstats:
      collection_interval: 30s
      auth_type: "serviceAccount"
      endpoint: "https://\${KUBE_NODE_NAME}:10250"
      insecure_skip_verify: true
      metric_groups:
        - container
        - pod
        - node

  exporters:
    otlp:
      endpoint: ${WFM_IP}:30417
      tls:
        insecure: true

    prometheus:
      endpoint: "0.0.0.0:8899"

    debug:
      verbosity: detailed

  processors:
    batch: {}

  service:
    pipelines:
      traces:
        receivers: [otlp]
        processors: [batch]
        exporters: [otlp, debug]

      metrics:
        receivers: [otlp, hostmetrics, kubeletstats]
        processors: [batch]
        exporters: [prometheus, debug]
EOF

  helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
  helm repo update

  helm install $OTEL_RELEASE open-telemetry/opentelemetry-collector --version 0.140.0 -f otel-values.yaml --namespace $NAMESPACE_OBSERVABILITY

  echo "üîß Patching OTEL Collector service to expose Prometheus metrics on NodePort 30999..."
  sudo kubectl patch svc otel-collector-opentelemetry-collector \
    -n $NAMESPACE_OBSERVABILITY \
    --type='json' \
    -p='[
      {
        "op": "add",
        "path": "/spec/ports/-",
        "value": {
          "name": "prometheus-metrics",
          "port": 8899,
          "protocol": "TCP",
          "targetPort": 8899,
          "nodePort": 30999
        }
      },
      {
        "op": "replace",
        "path": "/spec/type",
        "value": "NodePort"
      }
    ]'

  echo "‚úÖ OTEL Collector installed and Prometheus metrics exposed at NodePort 30999"
}

# Function to create observability namespace
create_observability_namespace() {
    echo "üîß Checking observability namespace..."
    
    if sudo kubectl get namespace $NAMESPACE_OBSERVABILITY >/dev/null 2>&1; then
        echo "‚úÖ Namespace '$NAMESPACE_OBSERVABILITY' already exists"
    else
        echo "üîß Creating namespace '$NAMESPACE_OBSERVABILITY'..."
        sudo kubectl create namespace $NAMESPACE_OBSERVABILITY
        echo "‚úÖ Namespace '$NAMESPACE_OBSERVABILITY' created successfully"
    fi
}

install_otel_collector_promtail_docker() {
  echo "Installing OTEL Collector v0.140.0 and Promtail v2.9.10 as Docker containers..."
  cd "$HOME/sandbox/pipeline/observability" || { echo '‚ùå observability dir missing'; exit 1; }
  
  # Fix Docker socket permissions for OTEL Collector access
  echo "Setting Docker socket permissions..."
  sudo chmod 666 /var/run/docker.sock
  
  # Create docker-compose.yml for observability stack
  cat <<EOF > docker-compose-observability.yml
version: '3.8'

services:
  promtail:
    image: grafana/promtail:2.9.10
    container_name: promtail
    volumes:
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./promtail-config.yml:/etc/promtail/config.yml
    command: -config.file=/etc/promtail/config.yml
    restart: unless-stopped
    network_mode: host

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.140.0
    container_name: otel-collector
    volumes:
      - ./otel-collector-config.yml:/etc/otel/config.yml
      - /var/run/docker.sock:/var/run/docker.sock
    command: --config=/etc/otel/config.yml
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8899:8899"   # Prometheus metrics
    restart: unless-stopped
    network_mode: host
    environment:
      - HOST_IP=\${HOST_IP:-127.0.0.1}
EOF

  # Create Promtail config
  cat <<EOF > promtail-config.yml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://${WFM_IP}:32100/loki/api/v1/push

scrape_configs:
  - job_name: docker-logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: dockerlogs
          __path__: /var/lib/docker/containers/*/*.log
EOF

  # Create OTEL Collector config with Docker stats and Jaeger export
  cat <<EOF > otel-collector-config.yml
receivers:
  # OTLP receiver for application traces/metrics
  otlp:
    protocols:
      http:
        endpoint: 0.0.0.0:4318
      grpc:
        endpoint: 0.0.0.0:4317

  # Host-level metrics
  hostmetrics:
    collection_interval: 30s
    scrapers:
      cpu:
      memory:
      disk:
      filesystem:
      load:
      network:
      processes:
      paging:

  # Docker container metrics
  docker_stats:
    endpoint: unix:///var/run/docker.sock
    collection_interval: 10s
    timeout: 5s
    api_version: "1.44"

exporters:
  # Send traces to Jaeger on WFM server
  otlp/jaeger:
    endpoint: ${WFM_IP}:4317
    tls:
      insecure: true

  # Expose metrics for Prometheus scraping
  prometheus:
    endpoint: "0.0.0.0:8899"

  # Debug output
  debug:
    verbosity: detailed

processors:
  batch:
    timeout: 10s
    send_batch_size: 1024

  # Add resource attributes
  resource:
    attributes:
      - key: device.type
        value: docker
        action: insert
      - key: device.ip
        value: \${HOST_IP}
        action: insert

service:
  pipelines:
    # Traces pipeline - send to Jaeger
    traces:
      receivers: [otlp]
      processors: [batch, resource]
      exporters: [otlp/jaeger, debug]

    # Metrics pipeline - expose for Prometheus
    metrics:
      receivers: [otlp, hostmetrics, docker_stats]
      processors: [batch, resource]
      exporters: [prometheus, debug]
EOF

  # Get host IP for resource attributes
  HOST_IP=$(hostname -I | awk '{print $1}')
  export HOST_IP

  # Start the observability stack
  docker compose -f docker-compose-observability.yml up -d
  
  echo "‚úÖ OTEL Collector v0.140.0 and Promtail v2.9.10 installed"
  echo "üì° OTLP gRPC: localhost:4317"
  echo "üì° OTLP HTTP: localhost:4318"
  echo "üìä Prometheus metrics: localhost:8899"
  echo "üîç Traces sent to Jaeger at: ${WFM_IP}:4317"
  
}


install_otel_collector_promtail_wrapper() {
  if [ "$DEVICE_TYPE" = "k3s" ]; then
    install_otel_collector_promtail  # Existing k8s-based installation
  else
    install_otel_collector_promtail_docker  # New Docker-based installation
  fi
}

uninstall_otel_collector_promtail_wrapper() {
  if [ "$DEVICE_TYPE" = "k3s" ]; then
    uninstall_otel_collector_promtail  # Existing k8s-based uninstallation
  else
    uninstall_otel_collector_promtail_docker  # New Docker-based uninstallation
  fi
}

uninstall_otel_collector_promtail_docker() {
  echo "üßπ Uninstalling Promtail and OTEL Collector containers..."
  cd "$HOME/sandbox/pipeline/observability" || { echo '‚ùå observability dir missing'; exit 1; }
  
  if [ -f "docker-compose-observability.yml" ]; then
    docker compose -f docker-compose-observability.yml down
    rm -f docker-compose-observability.yml promtail-config.yml otel-collector-config.yml
  fi
  
  echo "‚úÖ Cleanup complete."
}


install_otel_collector_promtail() {
  echo "Installing OTEL Collector and Promtail..."
  cd "$HOME/sandbox/pipeline/observability" || { echo '‚ùå observability dir missing'; exit 1; }
  create_observability_namespace
  install_promtail
  install_otel_collector
  echo "‚úÖ OTEL Collector and Promtail installation completed."
}

uninstall_otel_collector_promtail() {
  echo "üßπ Uninstalling Promtail and OTEL Collector..."
  cd "$HOME/sandbox/pipeline/observability" || { echo '‚ùå observability dir missing'; exit 1; }
   
   # Uninstall helm releases only if they exist
    for release in $PROMTAIL_RELEASE $OTEL_RELEASE; do
        if helm status $release -n "$NAMESPACE_OBSERVABILITY" >/dev/null 2>&1; then
            echo "üóëÔ∏è Uninstalling $release..."
            helm uninstall $release --namespace "$NAMESPACE_OBSERVABILITY"
        else
            echo "‚è≠Ô∏è $release not found, skipping..."
        fi
    done


  rm -f promtail-values.yaml otel-values.yaml
  echo "‚úÖ Cleanup complete."

}

cleanup_residual() {
  rm -rf "$HOME/sandbox"
  rm -rf "$HOME/symphony"
 }

create_device_rsa_certs() {
  CERT_DIR="$HOME/certs"

  # If certs exists but is not a directory, remove it
  if [ -e "$CERT_DIR" ] && [ ! -d "$CERT_DIR" ]; then
    echo "[WARNING] $CERT_DIR exists but is not a directory ‚Äî removing."
    rm -f "$CERT_DIR"
  fi

  mkdir -p "$CERT_DIR"
  cd "$CERT_DIR" || exit 1

  echo "Generating RSA device certs..."
  # Generate RSA private key (2048-bit)
  openssl genrsa -out device-private.key 2048

  # Generate self-signed certificate
  openssl req -new -x509 -key device-private.key -out device-public.crt -days 365 \
    -subj "/C=IN/ST=GGN/L=Sector 48/O=Margo/CN=margo-device"
  echo "‚úÖ RSA Cert generation has been completed."

									   
																				  
}

create_device_ecdsa_certs() {
  CERT_DIR="$HOME/certs"

  if [ ! -d "$CERT_DIR" ]; then
    echo "Cert directory not found. Creating $CERT_DIR ..."
    mkdir -p "$CERT_DIR"
  else
    echo "Using existing cert directory: $CERT_DIR"
  fi

  cd "$CERT_DIR" || exit 1
  echo "Generating ECDSA device certs..."
  # Generate ECDSA private key (P-256 curve)
  openssl ecparam -genkey -name prime256v1 -out device-ecdsa.key

  # Generate self-signed certificate
  openssl req -new -x509 -key device-ecdsa.key -out device-ecdsa.crt -days 365 \
    -subj "/C=IN/ST=GGN/L=Sector 48/O=Margo/CN=margo-device"
  echo "‚úÖ ECDSA Cert generation has been completed."

										 
																			   
}


show_menu() {
  echo "Choose an option:"
  echo "1) Install-prerequisites"
  echo "2) Uninstall-prerequisites"
  echo "3) Workload-Fleet-Management-client-Start(docker-compose-device)"
  echo "4) Workload-Fleet-Management-client-Stop(docker-compose-device)"
  echo "5) Workload-Fleet-Management-client-Start(k3s-device)"
  echo "6) Workload-Fleet-Management-client-Stop(k3s-device)"
  echo "7) Workload-Fleet-Management-client-Status"
  echo "8) OTEL-collector-promtail-installation"
  echo "9) OTEL-collector-promtail-uninstallation"
  echo "10) cleanup-residual"
  echo "11) create_device_rsa_certs"
  echo "12) create_device_ecdsa_certs"
  read -rp "Enter choice [1-12]: " choice
  case $choice in
    1) install_prerequisites;;
    2) uninstall_prerequisites;;
    3) start_device_agent_docker ;;
    4) stop_device_agent_docker ;;
    5) start_device_agent_kubernetes ;;
    6) stop_device_agent_kubernetes ;;
    7) show_status ;;
    8) install_otel_collector_promtail_wrapper ;;
    9) uninstall_otel_collector_promtail_wrapper ;;
    10) cleanup_residual;;
    11) create_device_rsa_certs ;;
    12) create_device_ecdsa_certs ;;
    *) echo "Invalid choice" ;;
  esac
}

# ----------------------------
# Main Script Execution
# ----------------------------
if [ -z "$1" ]; then
  show_menu
fi